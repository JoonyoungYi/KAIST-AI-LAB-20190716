{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Early Stop\n",
    "\n",
    "우리는 이번 실습에서 Early Stopping을 하는 방법에 대해서 배워보고자 합니다. Early Stop은 학습을 너무 많이 시키면 모델이 과적합(overfitting)될 수 있기 때문에 overfitting이 되지 않도록 적당한 Epoch에서 학습을 멈춰주는 정규화 방법을 말합니다. \n",
    "![earlystop](images/02_early_stop.png)\n",
    "이전 실습과 대부분의 코드는 동일합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed=seed)\n",
    "tf.random.set_random_seed(seed)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape([-1, 28 * 28])\n",
    "x_test = x_test.reshape([-1, 28 * 28])\n",
    "\n",
    "m = np.random.randint(0, high=60000, size=1100, dtype=np.int64)\n",
    "x_train = x_train[m]\n",
    "y_train = y_train[m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early Stopping을 하기 위해 필요한 작업은 전체 1100개의 트레이닝 데이터에서 Validation 데이터를 떼는 것입니다. Training 데이터 셋 내에서 Validation set을 떼어낸 후에 Validation set 없이 학습을 진행하다가 Validation Error가 가장 최적인 때의 모델 파라미터를 최종 모델로 사용합니다. 아래 코드로 1100개의 트레이닝 데이터에서 100개를 Validation set으로 떼어냅니다. 통상적으로 10%~20%를 validation으로 떼어내는데, validation set이 너무 작으면 test set을 대변하지 못할 수도 있고 또 validation set이 너무 크면 모델 성능이 저하될 수 있으므로 적당한 수준으로 validation set을 떼어내는 작업이 중요합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.arange(1100)\n",
    "np.random.shuffle(i)\n",
    "x_train = x_train[i]\n",
    "y_train = y_train[i]\n",
    "\n",
    "x_valid = x_train[:100]\n",
    "y_valid = y_train[:100]\n",
    "\n",
    "x_train = x_train[100:]\n",
    "y_train = y_train[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 나머지 부분들은 이전 실습과 동일하게 진행합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 28 * 28])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "n_units = [28 * 28, 512, 512, 10]\n",
    "\n",
    "weights, biases = [], []\n",
    "for i, (n_in, n_out) in enumerate(zip(n_units[:-1], n_units[1:])):\n",
    "    stddev = math.sqrt(2 / n_in) # Kaiming He Initialization\n",
    "    weight = tf.Variable(tf.random.truncated_normal([n_in, n_out], mean=0, stddev=stddev))\n",
    "    bias = tf.Variable(tf.zeros([n_out]))\n",
    "    weights.append(weight)\n",
    "    biases.append(bias)\n",
    "    \n",
    "layer = x \n",
    "for i, (weight, bias) in enumerate(zip(weights, biases)):\n",
    "    layer = tf.matmul(layer, weight) + bias\n",
    "    if i < len(weights) - 1:\n",
    "        layer = tf.nn.tanh(layer)        \n",
    "y_hat = layer\n",
    "\n",
    "y_hot = tf.one_hot(y, 10)\n",
    "costs = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        labels=y_hot, logits=y_hat)\n",
    "cross_entropy_loss = tf.reduce_mean(costs)\n",
    "loss = cross_entropy_loss \n",
    "\n",
    "accuracy = tf.count_nonzero(\n",
    "        tf.cast(tf.equal(tf.argmax(y_hot, 1), tf.argmax(y_hat, 1)),\n",
    "                tf.int64)) / tf.cast(tf.shape(y_hot)[0], tf.int64)\n",
    "\n",
    "extra_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_ops):\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    \n",
    "gpu_options = tf.GPUOptions()\n",
    "gpu_options.allow_growth = True\n",
    "session = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 Epoch별 train/valid/test accuracy, loss를 측정하기 위한 리스트 변수들을 선언합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy_values, valid_accuracy_values, test_accuracy_values = [], [], []\n",
    "train_loss_values, valid_loss_values, test_loss_values = [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습을 진행합니다. 총 1000번의 Epoch을 수행합니다. 총 100 Epoch 동안 성능 개선이 이뤄지지 않으면 Early Stop 합니다. Early stop은 accuracy 기준으로 진행합니다. (Quiz: 왜 Loss 기준으로 진행하지는 않을까요?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_valid_epoch_idx = 0\n",
    "max_valid_accuracy = 0.0\n",
    "final_test_accuracy = 0.0\n",
    "for epoch_idx in range(1, 1000 + 1):\n",
    "    session.run(\n",
    "            train_op,\n",
    "            feed_dict={\n",
    "                x: x_train,\n",
    "                y: y_train\n",
    "            })\n",
    "    \n",
    "    if epoch_idx % 10 == 0:\n",
    "        train_loss_value, train_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_train,\n",
    "                y: y_train\n",
    "            })\n",
    "        \n",
    "        valid_loss_value, valid_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_valid,\n",
    "                y: y_valid\n",
    "            })\n",
    "            \n",
    "        test_loss_value, test_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_test,\n",
    "                y: y_test\n",
    "            })\n",
    "\n",
    "        print(epoch_idx, '%.4f' % train_loss_value, '%.4f' % valid_loss_value, '%.4f' % test_loss_value, '%.4f' % train_accuracy_value, '%.4f' % valid_accuracy_value, '%.4f' % test_accuracy_value)\n",
    "        train_accuracy_values.append(train_accuracy_value)\n",
    "        valid_accuracy_values.append(valid_accuracy_value)\n",
    "        test_accuracy_values.append(test_accuracy_value)\n",
    "\n",
    "        train_loss_values.append(train_loss_value)\n",
    "        valid_loss_values.append(valid_loss_value)\n",
    "        test_loss_values.append(test_loss_value)\n",
    "        \n",
    "        if max_valid_accuracy < valid_accuracy_value:\n",
    "            max_valid_accuracy = valid_accuracy_value \n",
    "            max_valid_epoch_idx = epoch_idx\n",
    "            final_test_accuracy = test_accuracy_value\n",
    "            \n",
    "    # Early Stop\n",
    "    if max_valid_epoch_idx + 100 < epoch_idx:\n",
    "        break\n",
    "        \n",
    "print(max(test_accuracy_values))\n",
    "print(final_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 이 네트워크가 최고 88.34%의 Test Accuracy를 나타내고, Early Stop을 수행한 시점의 모델 성능은 88.05% 임을 확인할 수 있었습니다. 물론 최고 Test Accuracy와는 다소 차이가 있지만 Early Stopping을 통해 적당한 epoch 수를 자동으로 찾아낼 수 있었습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_accuracy_values, label=\"train\")\n",
    "plt.plot(valid_accuracy_values, label=\"valid\")\n",
    "plt.plot(test_accuracy_values, label=\"test\")\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch x10')\n",
    "plt.ylim([0.8, 1.0])\n",
    "plt.axhline(y=max(test_accuracy_values), color='r', linestyle='-')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_loss_values, label=\"train\")\n",
    "plt.plot(valid_loss_values, label=\"valid\")\n",
    "plt.plot(test_loss_values, label=\"test\")\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch x10')\n",
    "plt.axhline(y=min(test_loss_values), color='r', linestyle='-')\n",
    "plt.ylim([0, 0.6])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Set이 overfitting하기 시작하는 시점이 Test set이 overfitting하기 시작하는 시점과 상당히 유사한 것을 알 수 있습니다. \n",
    "\n",
    "다음 [실습](01_03_input_layer_stabilization.ipynb)에서는 Input Layer 안정화 방식에 대해서 배워보고자 합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
