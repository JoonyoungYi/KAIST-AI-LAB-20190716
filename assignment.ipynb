{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "(숙제 설명)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import cifar10, fashion_mnist\n",
    "import math \n",
    "\n",
    "seed = 777\n",
    "np.random.seed(seed)\n",
    "tf.random.set_random_seed(seed) # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def plot_fn(labels, train_loss_histories, train_acc_histories, test_acc_histories):\n",
    "    fig = plt.figure(figsize=(18,5))\n",
    "    ax1 = fig.add_subplot(1, 3, 1)\n",
    "    ax2 = fig.add_subplot(1, 3, 2)\n",
    "    ax3 = fig.add_subplot(1, 3, 3)\n",
    "    for label, train_loss_history, train_acc_history, test_acc_history in zip(labels, train_loss_histories, train_acc_histories, test_acc_histories):\n",
    "        ax1.plot(train_loss_history, label=str(label))\n",
    "        ax2.plot(train_acc_history, label=str(label))\n",
    "        ax3.plot(test_acc_history, label=str(label))\n",
    "        \n",
    "    ax1.set_xlabel('Batch #')\n",
    "    ax1.set_ylabel('Training Loss [entropy]')\n",
    "    ax2.set_xlabel('Batch #')\n",
    "    ax2.set_ylabel('Training Accuracy')\n",
    "    ax3.set_xlabel('Batch #')\n",
    "    ax3.set_ylabel('Test Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper paramenters\n",
    "batch_size = 1024\n",
    "epochs = 200\n",
    "aug_scale = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "(데이터 추후 결정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input shape: (1000, 32, 32, 3)\n",
      "Train label shape: (1000, 1)\n",
      "Test input shape: (10000, 32, 32, 3)\n",
      "Test label shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data() # cifar10.load_data()\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype(np.float32)[:1000] / 255. \n",
    "x_train += np.random.normal(0, 0.3, x_train.shape)\n",
    "y_train = y_train.astype(np.int64)[:1000]\n",
    "x_test = x_test.astype(np.float32) / 255.\n",
    "# x_test += np.random.normal(0, 0.3, x_test.shape)\n",
    "y_test = y_test.astype(np.int64)\n",
    "\n",
    "generator = ImageDataGenerator(rotation_range=20,\n",
    "                       width_shift_range=0.1,\n",
    "                       height_shift_range=0.1)\n",
    "\n",
    "data_flow = generator.flow(x_train, y_train,\n",
    "                            batch_size=batch_size)\n",
    "\n",
    "print(\"Train input shape: {}\".format(x_train.shape))\n",
    "print(\"Train label shape: {}\".format(y_train.shape))\n",
    "print(\"Test input shape: {}\".format(x_test.shape))\n",
    "print(\"Test label shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 정의\n",
    "from tensorflow.keras.layers import Flatten, Dense, Activation, Dropout, BatchNormalization\n",
    "\n",
    "def create_model(optimizer='sgd', init='glorot_uniform', drop_rate=0.0, batch_norm=False):\n",
    "    model = keras.Sequential()\n",
    "    # input layer\n",
    "    model.add(Flatten(input_shape=(32, 32, 3)))\n",
    "    # hidden layers\n",
    "    model.add(Dense(2048,kernel_initializer=init))\n",
    "    if batch_norm:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(drop_rate))\n",
    "    \n",
    "    model.add(Dense(2048,kernel_initializer=init))\n",
    "    if batch_norm:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(drop_rate))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(10,kernel_initializer=init))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0718 20:42:10.620637 139676478900032 deprecation.py:506] From /home/yongsu/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2048)              6293504   \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                20490     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 10,510,346\n",
      "Trainable params: 10,510,346\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model 생성\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "1000/1000 [==============================] - 0s 392us/sample - loss: 2.4359 - acc: 0.0920 - val_loss: 2.3092 - val_acc: 0.1018\n",
      "Epoch 2/200\n",
      "1000/1000 [==============================] - 0s 142us/sample - loss: 2.3116 - acc: 0.1050 - val_loss: 2.2928 - val_acc: 0.1251\n",
      "Epoch 3/200\n",
      "1000/1000 [==============================] - 0s 137us/sample - loss: 2.2860 - acc: 0.1280 - val_loss: 2.2841 - val_acc: 0.1356\n",
      "Epoch 4/200\n",
      "1000/1000 [==============================] - 0s 159us/sample - loss: 2.2709 - acc: 0.1460 - val_loss: 2.2761 - val_acc: 0.1413\n",
      "Epoch 5/200\n",
      "1000/1000 [==============================] - 0s 144us/sample - loss: 2.2575 - acc: 0.1550 - val_loss: 2.2683 - val_acc: 0.1477\n",
      "Epoch 6/200\n",
      "1000/1000 [==============================] - 0s 135us/sample - loss: 2.2446 - acc: 0.1680 - val_loss: 2.2608 - val_acc: 0.1515\n",
      "Epoch 7/200\n",
      "1000/1000 [==============================] - 0s 138us/sample - loss: 2.2321 - acc: 0.1790 - val_loss: 2.2535 - val_acc: 0.1558\n",
      "Epoch 8/200\n",
      "1000/1000 [==============================] - 0s 134us/sample - loss: 2.2200 - acc: 0.1850 - val_loss: 2.2464 - val_acc: 0.1609\n",
      "Epoch 9/200\n",
      "1000/1000 [==============================] - 0s 135us/sample - loss: 2.2081 - acc: 0.1940 - val_loss: 2.2397 - val_acc: 0.1639\n",
      "Epoch 10/200\n",
      "1000/1000 [==============================] - 0s 139us/sample - loss: 2.1966 - acc: 0.2090 - val_loss: 2.2332 - val_acc: 0.1683\n",
      "Epoch 11/200\n",
      "1000/1000 [==============================] - 0s 139us/sample - loss: 2.1853 - acc: 0.2180 - val_loss: 2.2269 - val_acc: 0.1739\n",
      "Epoch 12/200\n",
      "1000/1000 [==============================] - 0s 138us/sample - loss: 2.1742 - acc: 0.2230 - val_loss: 2.2209 - val_acc: 0.1780\n",
      "Epoch 13/200\n",
      "1000/1000 [==============================] - 0s 140us/sample - loss: 2.1634 - acc: 0.2280 - val_loss: 2.2150 - val_acc: 0.1804\n",
      "Epoch 14/200\n",
      "1000/1000 [==============================] - 0s 133us/sample - loss: 2.1528 - acc: 0.2380 - val_loss: 2.2094 - val_acc: 0.1828\n",
      "Epoch 15/200\n",
      "1000/1000 [==============================] - 0s 138us/sample - loss: 2.1425 - acc: 0.2470 - val_loss: 2.2040 - val_acc: 0.1866\n",
      "Epoch 16/200\n",
      "1000/1000 [==============================] - 0s 127us/sample - loss: 2.1324 - acc: 0.2550 - val_loss: 2.1987 - val_acc: 0.1911\n",
      "Epoch 17/200\n",
      "1000/1000 [==============================] - 0s 134us/sample - loss: 2.1225 - acc: 0.2640 - val_loss: 2.1936 - val_acc: 0.1932\n",
      "Epoch 18/200\n",
      "1000/1000 [==============================] - 0s 133us/sample - loss: 2.1128 - acc: 0.2740 - val_loss: 2.1887 - val_acc: 0.1971\n",
      "Epoch 19/200\n",
      "1000/1000 [==============================] - 0s 140us/sample - loss: 2.1032 - acc: 0.2850 - val_loss: 2.1839 - val_acc: 0.1998\n",
      "Epoch 20/200\n",
      "1000/1000 [==============================] - 0s 134us/sample - loss: 2.0939 - acc: 0.2920 - val_loss: 2.1793 - val_acc: 0.2029\n",
      "Epoch 21/200\n",
      "1000/1000 [==============================] - 0s 131us/sample - loss: 2.0846 - acc: 0.3050 - val_loss: 2.1748 - val_acc: 0.2046\n",
      "Epoch 22/200\n",
      "1000/1000 [==============================] - 0s 132us/sample - loss: 2.0756 - acc: 0.3150 - val_loss: 2.1704 - val_acc: 0.2075\n",
      "Epoch 23/200\n",
      "1000/1000 [==============================] - 0s 134us/sample - loss: 2.0666 - acc: 0.3180 - val_loss: 2.1661 - val_acc: 0.2094\n",
      "Epoch 24/200\n",
      "1000/1000 [==============================] - 0s 133us/sample - loss: 2.0578 - acc: 0.3220 - val_loss: 2.1619 - val_acc: 0.2116\n",
      "Epoch 25/200\n",
      "1000/1000 [==============================] - 0s 140us/sample - loss: 2.0491 - acc: 0.3290 - val_loss: 2.1579 - val_acc: 0.2141\n",
      "Epoch 26/200\n",
      "1000/1000 [==============================] - 0s 136us/sample - loss: 2.0405 - acc: 0.3390 - val_loss: 2.1539 - val_acc: 0.2169\n",
      "Epoch 27/200\n",
      "1000/1000 [==============================] - 0s 132us/sample - loss: 2.0320 - acc: 0.3500 - val_loss: 2.1500 - val_acc: 0.2186\n",
      "Epoch 28/200\n",
      "1000/1000 [==============================] - 0s 134us/sample - loss: 2.0236 - acc: 0.3570 - val_loss: 2.1463 - val_acc: 0.2208\n",
      "Epoch 29/200\n",
      "1000/1000 [==============================] - 0s 135us/sample - loss: 2.0153 - acc: 0.3590 - val_loss: 2.1427 - val_acc: 0.2232\n",
      "Epoch 30/200\n",
      "1000/1000 [==============================] - 0s 139us/sample - loss: 2.0072 - acc: 0.3680 - val_loss: 2.1392 - val_acc: 0.2250\n",
      "Epoch 31/200\n",
      "1000/1000 [==============================] - 0s 144us/sample - loss: 1.9992 - acc: 0.3740 - val_loss: 2.1357 - val_acc: 0.2271\n",
      "Epoch 32/200\n",
      "1000/1000 [==============================] - 0s 147us/sample - loss: 1.9913 - acc: 0.3760 - val_loss: 2.1323 - val_acc: 0.2294\n",
      "Epoch 33/200\n",
      "1000/1000 [==============================] - 0s 136us/sample - loss: 1.9835 - acc: 0.3780 - val_loss: 2.1290 - val_acc: 0.2322\n",
      "Epoch 34/200\n",
      "1000/1000 [==============================] - 0s 141us/sample - loss: 1.9757 - acc: 0.3860 - val_loss: 2.1258 - val_acc: 0.2346\n",
      "Epoch 35/200\n",
      "1000/1000 [==============================] - 0s 158us/sample - loss: 1.9681 - acc: 0.3900 - val_loss: 2.1226 - val_acc: 0.2360\n",
      "Epoch 36/200\n",
      "1000/1000 [==============================] - 0s 135us/sample - loss: 1.9606 - acc: 0.3930 - val_loss: 2.1195 - val_acc: 0.2382\n",
      "Epoch 37/200\n",
      "1000/1000 [==============================] - 0s 139us/sample - loss: 1.9532 - acc: 0.3960 - val_loss: 2.1165 - val_acc: 0.2397\n",
      "Epoch 38/200\n",
      "1000/1000 [==============================] - 0s 139us/sample - loss: 1.9459 - acc: 0.4040 - val_loss: 2.1135 - val_acc: 0.2407\n",
      "Epoch 39/200\n",
      "1000/1000 [==============================] - 0s 131us/sample - loss: 1.9386 - acc: 0.4140 - val_loss: 2.1106 - val_acc: 0.2424\n",
      "Epoch 40/200\n",
      "1000/1000 [==============================] - 0s 138us/sample - loss: 1.9313 - acc: 0.4190 - val_loss: 2.1078 - val_acc: 0.2438\n",
      "Epoch 41/200\n",
      "1000/1000 [==============================] - 0s 135us/sample - loss: 1.9242 - acc: 0.4280 - val_loss: 2.1051 - val_acc: 0.2455\n",
      "Epoch 42/200\n",
      "1000/1000 [==============================] - 0s 138us/sample - loss: 1.9171 - acc: 0.4330 - val_loss: 2.1024 - val_acc: 0.2460\n",
      "Epoch 43/200\n",
      "1000/1000 [==============================] - 0s 144us/sample - loss: 1.9101 - acc: 0.4340 - val_loss: 2.0997 - val_acc: 0.2475\n",
      "Epoch 44/200\n",
      "1000/1000 [==============================] - 0s 134us/sample - loss: 1.9031 - acc: 0.4380 - val_loss: 2.0971 - val_acc: 0.2486\n",
      "Epoch 45/200\n",
      "1000/1000 [==============================] - 0s 137us/sample - loss: 1.8962 - acc: 0.4460 - val_loss: 2.0945 - val_acc: 0.2495\n",
      "Epoch 46/200\n",
      "1000/1000 [==============================] - 0s 142us/sample - loss: 1.8894 - acc: 0.4540 - val_loss: 2.0920 - val_acc: 0.2504\n",
      "Epoch 47/200\n",
      "1000/1000 [==============================] - 0s 141us/sample - loss: 1.8826 - acc: 0.4570 - val_loss: 2.0896 - val_acc: 0.2514\n",
      "Epoch 48/200\n",
      "1000/1000 [==============================] - 0s 139us/sample - loss: 1.8759 - acc: 0.4630 - val_loss: 2.0872 - val_acc: 0.2526\n",
      "Epoch 49/200\n",
      "1000/1000 [==============================] - 0s 138us/sample - loss: 1.8693 - acc: 0.4700 - val_loss: 2.0848 - val_acc: 0.2537\n",
      "Epoch 50/200\n",
      "1000/1000 [==============================] - 0s 135us/sample - loss: 1.8627 - acc: 0.4800 - val_loss: 2.0825 - val_acc: 0.2545\n",
      "Epoch 51/200\n",
      "1000/1000 [==============================] - 0s 135us/sample - loss: 1.8562 - acc: 0.4830 - val_loss: 2.0803 - val_acc: 0.2547\n",
      "Epoch 52/200\n",
      "1000/1000 [==============================] - 0s 136us/sample - loss: 1.8497 - acc: 0.4850 - val_loss: 2.0781 - val_acc: 0.2547\n",
      "Epoch 53/200\n",
      "1000/1000 [==============================] - 0s 135us/sample - loss: 1.8433 - acc: 0.4870 - val_loss: 2.0759 - val_acc: 0.2560\n",
      "Epoch 54/200\n",
      "1000/1000 [==============================] - 0s 141us/sample - loss: 1.8370 - acc: 0.4920 - val_loss: 2.0737 - val_acc: 0.2577\n",
      "Epoch 55/200\n",
      "1000/1000 [==============================] - 0s 132us/sample - loss: 1.8307 - acc: 0.4930 - val_loss: 2.0717 - val_acc: 0.2592\n",
      "Epoch 56/200\n",
      "1000/1000 [==============================] - 0s 131us/sample - loss: 1.8244 - acc: 0.4970 - val_loss: 2.0696 - val_acc: 0.2595\n",
      "Epoch 57/200\n",
      "1000/1000 [==============================] - 0s 135us/sample - loss: 1.8182 - acc: 0.5040 - val_loss: 2.0676 - val_acc: 0.2602\n",
      "Epoch 58/200\n",
      "1000/1000 [==============================] - 0s 140us/sample - loss: 1.8120 - acc: 0.5100 - val_loss: 2.0657 - val_acc: 0.2606\n",
      "Epoch 59/200\n",
      "1000/1000 [==============================] - 0s 137us/sample - loss: 1.8058 - acc: 0.5090 - val_loss: 2.0637 - val_acc: 0.2610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/200\n",
      "1000/1000 [==============================] - 0s 135us/sample - loss: 1.7997 - acc: 0.5150 - val_loss: 2.0618 - val_acc: 0.2618\n",
      "Epoch 61/200\n",
      "1000/1000 [==============================] - 0s 144us/sample - loss: 1.7937 - acc: 0.5190 - val_loss: 2.0600 - val_acc: 0.2627\n",
      "Epoch 62/200\n",
      "1000/1000 [==============================] - 0s 147us/sample - loss: 1.7877 - acc: 0.5210 - val_loss: 2.0581 - val_acc: 0.2628\n",
      "Epoch 63/200\n",
      "1000/1000 [==============================] - 0s 132us/sample - loss: 1.7817 - acc: 0.5250 - val_loss: 2.0563 - val_acc: 0.2634\n",
      "Epoch 64/200\n",
      "1000/1000 [==============================] - 0s 133us/sample - loss: 1.7758 - acc: 0.5280 - val_loss: 2.0545 - val_acc: 0.2644\n",
      "Epoch 65/200\n",
      "1000/1000 [==============================] - 0s 142us/sample - loss: 1.7698 - acc: 0.5310 - val_loss: 2.0528 - val_acc: 0.2651\n",
      "Epoch 66/200\n",
      "1000/1000 [==============================] - 0s 159us/sample - loss: 1.7640 - acc: 0.5370 - val_loss: 2.0511 - val_acc: 0.2662\n",
      "Epoch 67/200\n",
      "1000/1000 [==============================] - 0s 149us/sample - loss: 1.7581 - acc: 0.5380 - val_loss: 2.0494 - val_acc: 0.2671\n",
      "Epoch 68/200\n",
      "1000/1000 [==============================] - 0s 144us/sample - loss: 1.7523 - acc: 0.5430 - val_loss: 2.0478 - val_acc: 0.2687\n",
      "Epoch 69/200\n",
      "1000/1000 [==============================] - 0s 143us/sample - loss: 1.7466 - acc: 0.5440 - val_loss: 2.0462 - val_acc: 0.2697\n",
      "Epoch 70/200\n",
      "1000/1000 [==============================] - 0s 144us/sample - loss: 1.7409 - acc: 0.5480 - val_loss: 2.0446 - val_acc: 0.2704\n",
      "Epoch 71/200\n",
      "1000/1000 [==============================] - 0s 147us/sample - loss: 1.7352 - acc: 0.5510 - val_loss: 2.0430 - val_acc: 0.2715\n",
      "Epoch 72/200\n",
      "1000/1000 [==============================] - 0s 141us/sample - loss: 1.7295 - acc: 0.5550 - val_loss: 2.0415 - val_acc: 0.2721\n",
      "Epoch 73/200\n",
      "1000/1000 [==============================] - 0s 134us/sample - loss: 1.7239 - acc: 0.5560 - val_loss: 2.0400 - val_acc: 0.2729\n",
      "Epoch 74/200\n",
      "1000/1000 [==============================] - 0s 136us/sample - loss: 1.7183 - acc: 0.5590 - val_loss: 2.0385 - val_acc: 0.2735\n",
      "Epoch 75/200\n",
      "1000/1000 [==============================] - 0s 140us/sample - loss: 1.7127 - acc: 0.5620 - val_loss: 2.0370 - val_acc: 0.2738\n",
      "Epoch 76/200\n",
      "1000/1000 [==============================] - 0s 152us/sample - loss: 1.7072 - acc: 0.5660 - val_loss: 2.0356 - val_acc: 0.2748\n",
      "Epoch 77/200\n",
      "1000/1000 [==============================] - 0s 135us/sample - loss: 1.7016 - acc: 0.5670 - val_loss: 2.0342 - val_acc: 0.2754\n",
      "Epoch 78/200\n",
      "1000/1000 [==============================] - 0s 140us/sample - loss: 1.6962 - acc: 0.5740 - val_loss: 2.0328 - val_acc: 0.2760\n",
      "Epoch 79/200\n",
      "1000/1000 [==============================] - 0s 137us/sample - loss: 1.6907 - acc: 0.5770 - val_loss: 2.0314 - val_acc: 0.2764\n",
      "Epoch 80/200\n",
      "1000/1000 [==============================] - 0s 141us/sample - loss: 1.6853 - acc: 0.5800 - val_loss: 2.0300 - val_acc: 0.2769\n",
      "Epoch 81/200\n",
      "1000/1000 [==============================] - 0s 144us/sample - loss: 1.6799 - acc: 0.5840 - val_loss: 2.0287 - val_acc: 0.2776\n",
      "Epoch 82/200\n",
      "1000/1000 [==============================] - 0s 161us/sample - loss: 1.6745 - acc: 0.5890 - val_loss: 2.0274 - val_acc: 0.2784\n",
      "Epoch 83/200\n",
      "1000/1000 [==============================] - 0s 140us/sample - loss: 1.6692 - acc: 0.5950 - val_loss: 2.0260 - val_acc: 0.2786\n",
      "Epoch 84/200\n",
      "1000/1000 [==============================] - 0s 135us/sample - loss: 1.6639 - acc: 0.5990 - val_loss: 2.0248 - val_acc: 0.2793\n",
      "Epoch 85/200\n",
      "1000/1000 [==============================] - 0s 145us/sample - loss: 1.6586 - acc: 0.6030 - val_loss: 2.0234 - val_acc: 0.2800\n",
      "Epoch 86/200\n",
      "1000/1000 [==============================] - 0s 155us/sample - loss: 1.6533 - acc: 0.6070 - val_loss: 2.0222 - val_acc: 0.2804\n",
      "Epoch 87/200\n",
      "1000/1000 [==============================] - 0s 171us/sample - loss: 1.6481 - acc: 0.6120 - val_loss: 2.0210 - val_acc: 0.2808\n",
      "Epoch 88/200\n",
      "1000/1000 [==============================] - 0s 147us/sample - loss: 1.6428 - acc: 0.6130 - val_loss: 2.0198 - val_acc: 0.2812\n",
      "Epoch 89/200\n",
      "1000/1000 [==============================] - 0s 149us/sample - loss: 1.6376 - acc: 0.6180 - val_loss: 2.0186 - val_acc: 0.2823\n",
      "Epoch 90/200\n",
      "1000/1000 [==============================] - 0s 153us/sample - loss: 1.6325 - acc: 0.6230 - val_loss: 2.0175 - val_acc: 0.2825\n",
      "Epoch 91/200\n",
      "1000/1000 [==============================] - 0s 148us/sample - loss: 1.6273 - acc: 0.6230 - val_loss: 2.0162 - val_acc: 0.2830\n",
      "Epoch 92/200\n",
      "1000/1000 [==============================] - 0s 137us/sample - loss: 1.6222 - acc: 0.6270 - val_loss: 2.0151 - val_acc: 0.2837\n",
      "Epoch 93/200\n",
      "1000/1000 [==============================] - 0s 153us/sample - loss: 1.6171 - acc: 0.6290 - val_loss: 2.0140 - val_acc: 0.2841\n",
      "Epoch 94/200\n",
      "1000/1000 [==============================] - 0s 156us/sample - loss: 1.6121 - acc: 0.6300 - val_loss: 2.0128 - val_acc: 0.2847\n",
      "Epoch 95/200\n",
      "1000/1000 [==============================] - 0s 146us/sample - loss: 1.6070 - acc: 0.6330 - val_loss: 2.0117 - val_acc: 0.2855\n",
      "Epoch 96/200\n",
      "1000/1000 [==============================] - 0s 134us/sample - loss: 1.6020 - acc: 0.6350 - val_loss: 2.0106 - val_acc: 0.2861\n",
      "Epoch 97/200\n",
      "1000/1000 [==============================] - 0s 145us/sample - loss: 1.5969 - acc: 0.6400 - val_loss: 2.0095 - val_acc: 0.2867\n",
      "Epoch 98/200\n",
      "1000/1000 [==============================] - 0s 145us/sample - loss: 1.5919 - acc: 0.6420 - val_loss: 2.0085 - val_acc: 0.2871\n",
      "Epoch 99/200\n",
      "1000/1000 [==============================] - 0s 143us/sample - loss: 1.5869 - acc: 0.6470 - val_loss: 2.0075 - val_acc: 0.2877\n",
      "Epoch 100/200\n",
      "1000/1000 [==============================] - 0s 141us/sample - loss: 1.5820 - acc: 0.6480 - val_loss: 2.0064 - val_acc: 0.2880\n",
      "Epoch 101/200\n",
      "1000/1000 [==============================] - 0s 146us/sample - loss: 1.5770 - acc: 0.6530 - val_loss: 2.0054 - val_acc: 0.2885\n",
      "Epoch 102/200\n",
      "1000/1000 [==============================] - 0s 143us/sample - loss: 1.5721 - acc: 0.6570 - val_loss: 2.0044 - val_acc: 0.2895\n",
      "Epoch 103/200\n",
      "1000/1000 [==============================] - 0s 160us/sample - loss: 1.5672 - acc: 0.6580 - val_loss: 2.0034 - val_acc: 0.2899\n",
      "Epoch 104/200\n",
      "1000/1000 [==============================] - 0s 164us/sample - loss: 1.5623 - acc: 0.6620 - val_loss: 2.0025 - val_acc: 0.2898\n",
      "Epoch 105/200\n",
      "1000/1000 [==============================] - 0s 158us/sample - loss: 1.5574 - acc: 0.6630 - val_loss: 2.0015 - val_acc: 0.2900\n",
      "Epoch 106/200\n",
      "1000/1000 [==============================] - 0s 137us/sample - loss: 1.5525 - acc: 0.6650 - val_loss: 2.0006 - val_acc: 0.2902\n",
      "Epoch 107/200\n",
      "1000/1000 [==============================] - 0s 131us/sample - loss: 1.5477 - acc: 0.6670 - val_loss: 1.9996 - val_acc: 0.2912\n",
      "Epoch 108/200\n",
      "1000/1000 [==============================] - 0s 131us/sample - loss: 1.5428 - acc: 0.6680 - val_loss: 1.9987 - val_acc: 0.2918\n",
      "Epoch 109/200\n",
      "1000/1000 [==============================] - 0s 131us/sample - loss: 1.5380 - acc: 0.6680 - val_loss: 1.9978 - val_acc: 0.2923\n",
      "Epoch 110/200\n",
      "1000/1000 [==============================] - 0s 137us/sample - loss: 1.5333 - acc: 0.6690 - val_loss: 1.9969 - val_acc: 0.2927\n",
      "Epoch 111/200\n",
      "1000/1000 [==============================] - 0s 141us/sample - loss: 1.5285 - acc: 0.6720 - val_loss: 1.9960 - val_acc: 0.2933\n",
      "Epoch 112/200\n",
      "1000/1000 [==============================] - 0s 148us/sample - loss: 1.5237 - acc: 0.6720 - val_loss: 1.9951 - val_acc: 0.2939\n",
      "Epoch 113/200\n",
      "1000/1000 [==============================] - 0s 142us/sample - loss: 1.5190 - acc: 0.6740 - val_loss: 1.9942 - val_acc: 0.2942\n",
      "Epoch 114/200\n",
      "1000/1000 [==============================] - 0s 135us/sample - loss: 1.5142 - acc: 0.6740 - val_loss: 1.9934 - val_acc: 0.2943\n",
      "Epoch 115/200\n",
      "1000/1000 [==============================] - 0s 138us/sample - loss: 1.5095 - acc: 0.6750 - val_loss: 1.9925 - val_acc: 0.2951\n",
      "Epoch 116/200\n",
      "1000/1000 [==============================] - 0s 141us/sample - loss: 1.5048 - acc: 0.6770 - val_loss: 1.9917 - val_acc: 0.2952\n",
      "Epoch 117/200\n",
      "1000/1000 [==============================] - 0s 147us/sample - loss: 1.5001 - acc: 0.6790 - val_loss: 1.9908 - val_acc: 0.2959\n",
      "Epoch 118/200\n",
      "1000/1000 [==============================] - 0s 157us/sample - loss: 1.4954 - acc: 0.6810 - val_loss: 1.9900 - val_acc: 0.2959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/200\n",
      "1000/1000 [==============================] - 0s 131us/sample - loss: 1.4908 - acc: 0.6830 - val_loss: 1.9892 - val_acc: 0.2963\n",
      "Epoch 120/200\n",
      "1000/1000 [==============================] - 0s 136us/sample - loss: 1.4861 - acc: 0.6860 - val_loss: 1.9884 - val_acc: 0.2965\n",
      "Epoch 121/200\n",
      "1000/1000 [==============================] - 0s 130us/sample - loss: 1.4815 - acc: 0.6870 - val_loss: 1.9876 - val_acc: 0.2961\n",
      "Epoch 122/200\n",
      "1000/1000 [==============================] - 0s 131us/sample - loss: 1.4768 - acc: 0.6870 - val_loss: 1.9868 - val_acc: 0.2963\n",
      "Epoch 123/200\n",
      "1000/1000 [==============================] - 0s 129us/sample - loss: 1.4722 - acc: 0.6900 - val_loss: 1.9860 - val_acc: 0.2969\n",
      "Epoch 124/200\n",
      "1000/1000 [==============================] - 0s 128us/sample - loss: 1.4676 - acc: 0.6930 - val_loss: 1.9852 - val_acc: 0.2967\n",
      "Epoch 125/200\n",
      "1000/1000 [==============================] - 0s 139us/sample - loss: 1.4630 - acc: 0.6930 - val_loss: 1.9845 - val_acc: 0.2968\n",
      "Epoch 126/200\n",
      "1000/1000 [==============================] - 0s 159us/sample - loss: 1.4584 - acc: 0.6950 - val_loss: 1.9837 - val_acc: 0.2968\n",
      "Epoch 127/200\n",
      "1000/1000 [==============================] - 0s 154us/sample - loss: 1.4539 - acc: 0.6980 - val_loss: 1.9830 - val_acc: 0.2963\n",
      "Epoch 128/200\n",
      "1000/1000 [==============================] - 0s 160us/sample - loss: 1.4493 - acc: 0.7060 - val_loss: 1.9822 - val_acc: 0.2968\n",
      "Epoch 129/200\n",
      "1000/1000 [==============================] - 0s 162us/sample - loss: 1.4448 - acc: 0.7090 - val_loss: 1.9815 - val_acc: 0.2971\n",
      "Epoch 130/200\n",
      "1000/1000 [==============================] - 0s 132us/sample - loss: 1.4402 - acc: 0.7110 - val_loss: 1.9808 - val_acc: 0.2973\n",
      "Epoch 131/200\n",
      "1000/1000 [==============================] - 0s 140us/sample - loss: 1.4357 - acc: 0.7120 - val_loss: 1.9801 - val_acc: 0.2973\n",
      "Epoch 132/200\n",
      "1000/1000 [==============================] - 0s 148us/sample - loss: 1.4312 - acc: 0.7130 - val_loss: 1.9793 - val_acc: 0.2975\n",
      "Epoch 133/200\n",
      "1000/1000 [==============================] - 0s 144us/sample - loss: 1.4267 - acc: 0.7160 - val_loss: 1.9787 - val_acc: 0.2976\n",
      "Epoch 134/200\n",
      "1000/1000 [==============================] - 0s 134us/sample - loss: 1.4222 - acc: 0.7200 - val_loss: 1.9780 - val_acc: 0.2976\n",
      "Epoch 135/200\n",
      "1000/1000 [==============================] - 0s 131us/sample - loss: 1.4178 - acc: 0.7250 - val_loss: 1.9773 - val_acc: 0.2978\n",
      "Epoch 136/200\n",
      "1000/1000 [==============================] - 0s 134us/sample - loss: 1.4133 - acc: 0.7250 - val_loss: 1.9766 - val_acc: 0.2979\n",
      "Epoch 137/200\n",
      "1000/1000 [==============================] - 0s 153us/sample - loss: 1.4089 - acc: 0.7260 - val_loss: 1.9759 - val_acc: 0.2981\n",
      "Epoch 138/200\n",
      "1000/1000 [==============================] - 0s 140us/sample - loss: 1.4044 - acc: 0.7300 - val_loss: 1.9753 - val_acc: 0.2985\n",
      "Epoch 139/200\n",
      "1000/1000 [==============================] - 0s 151us/sample - loss: 1.4000 - acc: 0.7310 - val_loss: 1.9746 - val_acc: 0.2989\n",
      "Epoch 140/200\n",
      "1000/1000 [==============================] - 0s 156us/sample - loss: 1.3956 - acc: 0.7330 - val_loss: 1.9739 - val_acc: 0.2991\n",
      "Epoch 141/200\n",
      "1000/1000 [==============================] - 0s 149us/sample - loss: 1.3912 - acc: 0.7330 - val_loss: 1.9733 - val_acc: 0.2994\n",
      "Epoch 142/200\n",
      "1000/1000 [==============================] - 0s 135us/sample - loss: 1.3868 - acc: 0.7330 - val_loss: 1.9727 - val_acc: 0.2997\n",
      "Epoch 143/200\n",
      "1000/1000 [==============================] - 0s 154us/sample - loss: 1.3824 - acc: 0.7330 - val_loss: 1.9721 - val_acc: 0.2998\n",
      "Epoch 144/200\n",
      "1000/1000 [==============================] - 0s 135us/sample - loss: 1.3780 - acc: 0.7330 - val_loss: 1.9714 - val_acc: 0.3004\n",
      "Epoch 145/200\n",
      "1000/1000 [==============================] - 0s 137us/sample - loss: 1.3737 - acc: 0.7340 - val_loss: 1.9708 - val_acc: 0.3002\n",
      "Epoch 146/200\n",
      "1000/1000 [==============================] - 0s 141us/sample - loss: 1.3693 - acc: 0.7360 - val_loss: 1.9702 - val_acc: 0.3006\n",
      "Epoch 147/200\n",
      "1000/1000 [==============================] - 0s 136us/sample - loss: 1.3650 - acc: 0.7380 - val_loss: 1.9697 - val_acc: 0.3010\n",
      "Epoch 148/200\n",
      "1000/1000 [==============================] - 0s 134us/sample - loss: 1.3606 - acc: 0.7380 - val_loss: 1.9690 - val_acc: 0.3014\n",
      "Epoch 149/200\n",
      "1000/1000 [==============================] - 0s 129us/sample - loss: 1.3563 - acc: 0.7380 - val_loss: 1.9685 - val_acc: 0.3011\n",
      "Epoch 150/200\n",
      "1000/1000 [==============================] - 0s 148us/sample - loss: 1.3520 - acc: 0.7430 - val_loss: 1.9679 - val_acc: 0.3005\n",
      "Epoch 151/200\n",
      "1000/1000 [==============================] - 0s 139us/sample - loss: 1.3477 - acc: 0.7440 - val_loss: 1.9674 - val_acc: 0.3007\n",
      "Epoch 152/200\n",
      "1000/1000 [==============================] - 0s 138us/sample - loss: 1.3434 - acc: 0.7470 - val_loss: 1.9668 - val_acc: 0.3011\n",
      "Epoch 153/200\n",
      "1000/1000 [==============================] - 0s 136us/sample - loss: 1.3391 - acc: 0.7490 - val_loss: 1.9663 - val_acc: 0.3014\n",
      "Epoch 154/200\n",
      "1000/1000 [==============================] - 0s 136us/sample - loss: 1.3348 - acc: 0.7500 - val_loss: 1.9657 - val_acc: 0.3010\n",
      "Epoch 155/200\n",
      "1000/1000 [==============================] - 0s 136us/sample - loss: 1.3305 - acc: 0.7510 - val_loss: 1.9652 - val_acc: 0.3009\n",
      "Epoch 156/200\n",
      "1000/1000 [==============================] - 0s 134us/sample - loss: 1.3262 - acc: 0.7510 - val_loss: 1.9646 - val_acc: 0.3015\n",
      "Epoch 157/200\n",
      "1000/1000 [==============================] - 0s 136us/sample - loss: 1.3220 - acc: 0.7530 - val_loss: 1.9642 - val_acc: 0.3013\n",
      "Epoch 158/200\n",
      "1000/1000 [==============================] - 0s 137us/sample - loss: 1.3177 - acc: 0.7580 - val_loss: 1.9636 - val_acc: 0.3014\n",
      "Epoch 159/200\n",
      "1000/1000 [==============================] - 0s 133us/sample - loss: 1.3135 - acc: 0.7610 - val_loss: 1.9632 - val_acc: 0.3018\n",
      "Epoch 160/200\n",
      "1000/1000 [==============================] - 0s 134us/sample - loss: 1.3093 - acc: 0.7620 - val_loss: 1.9626 - val_acc: 0.3020\n",
      "Epoch 161/200\n",
      "1000/1000 [==============================] - 0s 137us/sample - loss: 1.3050 - acc: 0.7640 - val_loss: 1.9621 - val_acc: 0.3022\n",
      "Epoch 162/200\n",
      "1000/1000 [==============================] - 0s 139us/sample - loss: 1.3008 - acc: 0.7660 - val_loss: 1.9616 - val_acc: 0.3024\n",
      "Epoch 163/200\n",
      "1000/1000 [==============================] - 0s 140us/sample - loss: 1.2966 - acc: 0.7690 - val_loss: 1.9611 - val_acc: 0.3029\n",
      "Epoch 164/200\n",
      "1000/1000 [==============================] - 0s 146us/sample - loss: 1.2924 - acc: 0.7710 - val_loss: 1.9606 - val_acc: 0.3030\n",
      "Epoch 165/200\n",
      "1000/1000 [==============================] - 0s 141us/sample - loss: 1.2883 - acc: 0.7710 - val_loss: 1.9601 - val_acc: 0.3034\n",
      "Epoch 166/200\n",
      "1000/1000 [==============================] - 0s 142us/sample - loss: 1.2841 - acc: 0.7720 - val_loss: 1.9597 - val_acc: 0.3035\n",
      "Epoch 167/200\n",
      "1000/1000 [==============================] - 0s 144us/sample - loss: 1.2799 - acc: 0.7740 - val_loss: 1.9592 - val_acc: 0.3034\n",
      "Epoch 168/200\n",
      "1000/1000 [==============================] - 0s 138us/sample - loss: 1.2758 - acc: 0.7750 - val_loss: 1.9588 - val_acc: 0.3037\n",
      "Epoch 169/200\n",
      "1000/1000 [==============================] - 0s 130us/sample - loss: 1.2716 - acc: 0.7750 - val_loss: 1.9583 - val_acc: 0.3042\n",
      "Epoch 170/200\n",
      "1000/1000 [==============================] - 0s 129us/sample - loss: 1.2675 - acc: 0.7780 - val_loss: 1.9579 - val_acc: 0.3039\n",
      "Epoch 171/200\n",
      "1000/1000 [==============================] - 0s 131us/sample - loss: 1.2633 - acc: 0.7780 - val_loss: 1.9574 - val_acc: 0.3043\n",
      "Epoch 172/200\n",
      "1000/1000 [==============================] - 0s 142us/sample - loss: 1.2592 - acc: 0.7800 - val_loss: 1.9570 - val_acc: 0.3046\n",
      "Epoch 173/200\n",
      "1000/1000 [==============================] - 0s 132us/sample - loss: 1.2551 - acc: 0.7800 - val_loss: 1.9565 - val_acc: 0.3045\n",
      "Epoch 174/200\n",
      "1000/1000 [==============================] - 0s 138us/sample - loss: 1.2510 - acc: 0.7840 - val_loss: 1.9561 - val_acc: 0.3047\n",
      "Epoch 175/200\n",
      "1000/1000 [==============================] - 0s 132us/sample - loss: 1.2469 - acc: 0.7840 - val_loss: 1.9556 - val_acc: 0.3047\n",
      "Epoch 176/200\n",
      "1000/1000 [==============================] - 0s 133us/sample - loss: 1.2428 - acc: 0.7880 - val_loss: 1.9553 - val_acc: 0.3048\n",
      "Epoch 177/200\n",
      "1000/1000 [==============================] - 0s 144us/sample - loss: 1.2387 - acc: 0.7880 - val_loss: 1.9548 - val_acc: 0.3048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/200\n",
      "1000/1000 [==============================] - 0s 147us/sample - loss: 1.2347 - acc: 0.7920 - val_loss: 1.9544 - val_acc: 0.3046\n",
      "Epoch 179/200\n",
      "1000/1000 [==============================] - 0s 138us/sample - loss: 1.2306 - acc: 0.7940 - val_loss: 1.9539 - val_acc: 0.3046\n",
      "Epoch 180/200\n",
      "1000/1000 [==============================] - 0s 144us/sample - loss: 1.2266 - acc: 0.7960 - val_loss: 1.9536 - val_acc: 0.3049\n",
      "Epoch 181/200\n",
      "1000/1000 [==============================] - 0s 145us/sample - loss: 1.2225 - acc: 0.7990 - val_loss: 1.9531 - val_acc: 0.3051\n",
      "Epoch 182/200\n",
      "1000/1000 [==============================] - 0s 141us/sample - loss: 1.2185 - acc: 0.8020 - val_loss: 1.9528 - val_acc: 0.3047\n",
      "Epoch 183/200\n",
      "1000/1000 [==============================] - 0s 153us/sample - loss: 1.2145 - acc: 0.8020 - val_loss: 1.9523 - val_acc: 0.3047\n",
      "Epoch 184/200\n",
      "1000/1000 [==============================] - 0s 162us/sample - loss: 1.2105 - acc: 0.8030 - val_loss: 1.9521 - val_acc: 0.3051\n",
      "Epoch 185/200\n",
      "1000/1000 [==============================] - 0s 137us/sample - loss: 1.2064 - acc: 0.8050 - val_loss: 1.9515 - val_acc: 0.3051\n",
      "Epoch 186/200\n",
      "1000/1000 [==============================] - 0s 138us/sample - loss: 1.2024 - acc: 0.8070 - val_loss: 1.9513 - val_acc: 0.3051\n",
      "Epoch 187/200\n",
      "1000/1000 [==============================] - 0s 148us/sample - loss: 1.1984 - acc: 0.8100 - val_loss: 1.9507 - val_acc: 0.3056\n",
      "Epoch 188/200\n",
      "1000/1000 [==============================] - 0s 138us/sample - loss: 1.1944 - acc: 0.8100 - val_loss: 1.9506 - val_acc: 0.3052\n",
      "Epoch 189/200\n",
      "1000/1000 [==============================] - 0s 136us/sample - loss: 1.1905 - acc: 0.8140 - val_loss: 1.9499 - val_acc: 0.3056\n",
      "Epoch 190/200\n",
      "1000/1000 [==============================] - 0s 137us/sample - loss: 1.1865 - acc: 0.8150 - val_loss: 1.9499 - val_acc: 0.3051\n",
      "Epoch 191/200\n",
      "1000/1000 [==============================] - 0s 136us/sample - loss: 1.1825 - acc: 0.8150 - val_loss: 1.9492 - val_acc: 0.3052\n",
      "Epoch 192/200\n",
      "1000/1000 [==============================] - 0s 145us/sample - loss: 1.1786 - acc: 0.8160 - val_loss: 1.9492 - val_acc: 0.3052\n",
      "Epoch 193/200\n",
      "1000/1000 [==============================] - 0s 135us/sample - loss: 1.1746 - acc: 0.8180 - val_loss: 1.9484 - val_acc: 0.3055\n",
      "Epoch 194/200\n",
      "1000/1000 [==============================] - 0s 132us/sample - loss: 1.1707 - acc: 0.8190 - val_loss: 1.9485 - val_acc: 0.3050\n",
      "Epoch 195/200\n",
      "1000/1000 [==============================] - 0s 154us/sample - loss: 1.1667 - acc: 0.8200 - val_loss: 1.9477 - val_acc: 0.3057\n",
      "Epoch 196/200\n",
      "1000/1000 [==============================] - 0s 153us/sample - loss: 1.1628 - acc: 0.8240 - val_loss: 1.9478 - val_acc: 0.3055\n",
      "Epoch 197/200\n",
      "1000/1000 [==============================] - 0s 132us/sample - loss: 1.1589 - acc: 0.8250 - val_loss: 1.9471 - val_acc: 0.3061\n",
      "Epoch 198/200\n",
      "1000/1000 [==============================] - 0s 131us/sample - loss: 1.1549 - acc: 0.8280 - val_loss: 1.9471 - val_acc: 0.3052\n",
      "Epoch 199/200\n",
      "1000/1000 [==============================] - 0s 140us/sample - loss: 1.1510 - acc: 0.8340 - val_loss: 1.9464 - val_acc: 0.3054\n",
      "Epoch 200/200\n",
      "1000/1000 [==============================] - 0s 146us/sample - loss: 1.1471 - acc: 0.8370 - val_loss: 1.9465 - val_acc: 0.3050\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "# hist = model.fit_generator(data_flow, epochs=epochs, steps_per_epoch=math.ceil(len(x_train) / batch_size), validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 51us/sample - loss: 1.9465 - acc: 0.3050\n",
      "Test accuracy: 0.3050\n"
     ]
    }
   ],
   "source": [
    "# test data evaluation\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy: {:.4f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot_fn(['CIFAR-10'], [hist.history['loss']], [hist.history['acc']], [hist.history['val_acc']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2048)              6293504   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                20490     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 10,526,730\n",
      "Trainable params: 10,518,538\n",
      "Non-trainable params: 8,192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = create_model(optimizer='adam', drop_rate=0.5, batch_norm=True)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "5/5 [==============================] - 2s 456ms/step - loss: 2.7726 - acc: 0.2230 - val_loss: 6.2408 - val_acc: 0.1007\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 1s 291ms/step - loss: 2.2252 - acc: 0.3156 - val_loss: 6.2373 - val_acc: 0.1266\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 2s 306ms/step - loss: 2.0000 - acc: 0.3502 - val_loss: 7.4007 - val_acc: 0.1295\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 1s 297ms/step - loss: 1.8460 - acc: 0.3904 - val_loss: 6.9559 - val_acc: 0.1333\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 2s 337ms/step - loss: 1.7499 - acc: 0.4050 - val_loss: 7.1709 - val_acc: 0.1275\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 2s 306ms/step - loss: 1.6740 - acc: 0.4340 - val_loss: 6.7684 - val_acc: 0.1421\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 2s 305ms/step - loss: 1.5858 - acc: 0.4512 - val_loss: 5.4447 - val_acc: 0.1613\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 1s 294ms/step - loss: 1.5408 - acc: 0.4752 - val_loss: 4.1799 - val_acc: 0.2224\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 2s 304ms/step - loss: 1.4519 - acc: 0.4922 - val_loss: 5.1981 - val_acc: 0.1684\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 1s 297ms/step - loss: 1.4162 - acc: 0.5068 - val_loss: 3.9346 - val_acc: 0.2523\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 1.3947 - acc: 0.5084 - val_loss: 3.8379 - val_acc: 0.2185\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 1s 292ms/step - loss: 1.3422 - acc: 0.5350 - val_loss: 3.6017 - val_acc: 0.2550\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 2s 302ms/step - loss: 1.2876 - acc: 0.5518 - val_loss: 3.5050 - val_acc: 0.2470\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 2s 302ms/step - loss: 1.2462 - acc: 0.5510 - val_loss: 2.7742 - val_acc: 0.2758\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 2s 301ms/step - loss: 1.1820 - acc: 0.5816 - val_loss: 2.9365 - val_acc: 0.2890\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 2s 303ms/step - loss: 1.1696 - acc: 0.5912 - val_loss: 2.7865 - val_acc: 0.2928\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 2s 301ms/step - loss: 1.1473 - acc: 0.5884 - val_loss: 2.9302 - val_acc: 0.2719\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 2s 304ms/step - loss: 1.0870 - acc: 0.6126 - val_loss: 2.6146 - val_acc: 0.2983\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 2s 306ms/step - loss: 1.0524 - acc: 0.6240 - val_loss: 2.7285 - val_acc: 0.2941\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 1s 292ms/step - loss: 1.0374 - acc: 0.6256 - val_loss: 2.6440 - val_acc: 0.3106\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.9901 - acc: 0.6394 - val_loss: 2.7328 - val_acc: 0.2872\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.9774 - acc: 0.6468 - val_loss: 2.6320 - val_acc: 0.2962\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 2s 305ms/step - loss: 0.9436 - acc: 0.6674 - val_loss: 2.6596 - val_acc: 0.2944\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 2s 301ms/step - loss: 0.9395 - acc: 0.6690 - val_loss: 2.5751 - val_acc: 0.3126\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.8958 - acc: 0.6744 - val_loss: 2.5619 - val_acc: 0.3104\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 2s 304ms/step - loss: 0.8841 - acc: 0.6886 - val_loss: 2.5865 - val_acc: 0.3106\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 1s 299ms/step - loss: 0.8538 - acc: 0.7022 - val_loss: 2.6563 - val_acc: 0.2938\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 1s 298ms/step - loss: 0.8213 - acc: 0.7108 - val_loss: 2.7749 - val_acc: 0.2810\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 2s 307ms/step - loss: 0.8225 - acc: 0.7108 - val_loss: 2.8269 - val_acc: 0.2728\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 2s 301ms/step - loss: 0.7923 - acc: 0.7206 - val_loss: 2.6987 - val_acc: 0.2941\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 2s 301ms/step - loss: 0.7540 - acc: 0.7384 - val_loss: 2.6032 - val_acc: 0.3106\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 1s 299ms/step - loss: 0.7610 - acc: 0.7342 - val_loss: 2.4281 - val_acc: 0.3214\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 2s 306ms/step - loss: 0.7213 - acc: 0.7432 - val_loss: 2.5641 - val_acc: 0.3126\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 1s 298ms/step - loss: 0.7118 - acc: 0.7470 - val_loss: 2.8524 - val_acc: 0.2908\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 1s 298ms/step - loss: 0.6865 - acc: 0.7600 - val_loss: 2.4528 - val_acc: 0.3370\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 2s 303ms/step - loss: 0.6813 - acc: 0.7644 - val_loss: 2.7441 - val_acc: 0.3080\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - 2s 322ms/step - loss: 0.6432 - acc: 0.7754 - val_loss: 2.5760 - val_acc: 0.3321\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 0.6685 - acc: 0.7706 - val_loss: 2.6987 - val_acc: 0.3162\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.6446 - acc: 0.7716 - val_loss: 2.5092 - val_acc: 0.3429\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 2s 300ms/step - loss: 0.6218 - acc: 0.7816 - val_loss: 2.5247 - val_acc: 0.3486\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 2s 300ms/step - loss: 0.6288 - acc: 0.7806 - val_loss: 2.6633 - val_acc: 0.3245\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 2s 303ms/step - loss: 0.5969 - acc: 0.7966 - val_loss: 2.6420 - val_acc: 0.3254\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 1s 298ms/step - loss: 0.5799 - acc: 0.8026 - val_loss: 2.5293 - val_acc: 0.3376\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 2s 329ms/step - loss: 0.5633 - acc: 0.8052 - val_loss: 2.6330 - val_acc: 0.3450\n",
      "Epoch 45/200\n",
      "5/5 [==============================] - 1s 295ms/step - loss: 0.5530 - acc: 0.8114 - val_loss: 2.7905 - val_acc: 0.3294\n",
      "Epoch 46/200\n",
      "5/5 [==============================] - 2s 301ms/step - loss: 0.5622 - acc: 0.7982 - val_loss: 2.7480 - val_acc: 0.3430\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - 2s 306ms/step - loss: 0.5495 - acc: 0.8078 - val_loss: 2.7958 - val_acc: 0.3377\n",
      "Epoch 48/200\n",
      "5/5 [==============================] - 1s 293ms/step - loss: 0.5117 - acc: 0.8238 - val_loss: 3.0124 - val_acc: 0.3191\n",
      "Epoch 49/200\n",
      "5/5 [==============================] - 2s 303ms/step - loss: 0.5123 - acc: 0.8268 - val_loss: 2.8657 - val_acc: 0.3171\n",
      "Epoch 50/200\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.5009 - acc: 0.8302 - val_loss: 2.6275 - val_acc: 0.3392\n",
      "Epoch 51/200\n",
      "5/5 [==============================] - 2s 318ms/step - loss: 0.4809 - acc: 0.8346 - val_loss: 2.7872 - val_acc: 0.3368\n",
      "Epoch 52/200\n",
      "5/5 [==============================] - 1s 296ms/step - loss: 0.4721 - acc: 0.8384 - val_loss: 2.9043 - val_acc: 0.3336\n",
      "Epoch 53/200\n",
      "5/5 [==============================] - 1s 299ms/step - loss: 0.4739 - acc: 0.8370 - val_loss: 2.6896 - val_acc: 0.3499\n",
      "Epoch 54/200\n",
      "5/5 [==============================] - 2s 308ms/step - loss: 0.4749 - acc: 0.8374 - val_loss: 2.6268 - val_acc: 0.3606\n",
      "Epoch 55/200\n",
      "5/5 [==============================] - 1s 299ms/step - loss: 0.4689 - acc: 0.8410 - val_loss: 2.8814 - val_acc: 0.3327\n",
      "Epoch 56/200\n",
      "5/5 [==============================] - 2s 307ms/step - loss: 0.4338 - acc: 0.8534 - val_loss: 2.6389 - val_acc: 0.3502\n",
      "Epoch 57/200\n",
      "5/5 [==============================] - 2s 303ms/step - loss: 0.4525 - acc: 0.8484 - val_loss: 2.7865 - val_acc: 0.3408\n",
      "Epoch 58/200\n",
      "5/5 [==============================] - 2s 320ms/step - loss: 0.4288 - acc: 0.8538 - val_loss: 2.7286 - val_acc: 0.3441\n",
      "Epoch 59/200\n",
      "5/5 [==============================] - 1s 294ms/step - loss: 0.4313 - acc: 0.8574 - val_loss: 2.7888 - val_acc: 0.3429\n",
      "Epoch 60/200\n",
      "5/5 [==============================] - 2s 300ms/step - loss: 0.4243 - acc: 0.8554 - val_loss: 2.8362 - val_acc: 0.3417\n",
      "Epoch 61/200\n",
      "5/5 [==============================] - 2s 301ms/step - loss: 0.4215 - acc: 0.8534 - val_loss: 2.9194 - val_acc: 0.3224\n",
      "Epoch 62/200\n",
      "5/5 [==============================] - 2s 305ms/step - loss: 0.3856 - acc: 0.8716 - val_loss: 3.1060 - val_acc: 0.3087\n",
      "Epoch 63/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 2s 304ms/step - loss: 0.3982 - acc: 0.8620 - val_loss: 2.8573 - val_acc: 0.3390\n",
      "Epoch 64/200\n",
      "5/5 [==============================] - 2s 318ms/step - loss: 0.3926 - acc: 0.8654 - val_loss: 2.6755 - val_acc: 0.3588\n",
      "Epoch 65/200\n",
      "5/5 [==============================] - 2s 303ms/step - loss: 0.3807 - acc: 0.8720 - val_loss: 2.7428 - val_acc: 0.3517\n",
      "Epoch 66/200\n",
      "5/5 [==============================] - 1s 299ms/step - loss: 0.3674 - acc: 0.8762 - val_loss: 2.7834 - val_acc: 0.3534\n",
      "Epoch 67/200\n",
      "5/5 [==============================] - 1s 298ms/step - loss: 0.3642 - acc: 0.8768 - val_loss: 2.7519 - val_acc: 0.3615\n",
      "Epoch 68/200\n",
      "5/5 [==============================] - 2s 301ms/step - loss: 0.3649 - acc: 0.8756 - val_loss: 2.8175 - val_acc: 0.3460\n",
      "Epoch 69/200\n",
      "5/5 [==============================] - 2s 303ms/step - loss: 0.3568 - acc: 0.8810 - val_loss: 2.8748 - val_acc: 0.3456\n",
      "Epoch 70/200\n",
      "5/5 [==============================] - 2s 302ms/step - loss: 0.3479 - acc: 0.8806 - val_loss: 2.8618 - val_acc: 0.3568\n",
      "Epoch 71/200\n",
      "5/5 [==============================] - 2s 320ms/step - loss: 0.3328 - acc: 0.8880 - val_loss: 3.0870 - val_acc: 0.3430\n",
      "Epoch 72/200\n",
      "5/5 [==============================] - 2s 302ms/step - loss: 0.3388 - acc: 0.8886 - val_loss: 2.8279 - val_acc: 0.3687\n",
      "Epoch 73/200\n",
      "5/5 [==============================] - 2s 306ms/step - loss: 0.3398 - acc: 0.8882 - val_loss: 2.9098 - val_acc: 0.3590\n",
      "Epoch 74/200\n",
      "5/5 [==============================] - 1s 293ms/step - loss: 0.3369 - acc: 0.8826 - val_loss: 2.7319 - val_acc: 0.3722\n",
      "Epoch 75/200\n",
      "5/5 [==============================] - 2s 301ms/step - loss: 0.3207 - acc: 0.8930 - val_loss: 3.0101 - val_acc: 0.3562\n",
      "Epoch 76/200\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3272 - acc: 0.8906 - val_loss: 2.9739 - val_acc: 0.3595\n",
      "Epoch 77/200\n",
      "5/5 [==============================] - 1s 294ms/step - loss: 0.3117 - acc: 0.8960 - val_loss: 2.8839 - val_acc: 0.3647\n",
      "Epoch 78/200\n",
      "5/5 [==============================] - 2s 301ms/step - loss: 0.2914 - acc: 0.9022 - val_loss: 3.0977 - val_acc: 0.3438\n",
      "Epoch 79/200\n",
      "5/5 [==============================] - 1s 300ms/step - loss: 0.2966 - acc: 0.8984 - val_loss: 2.8327 - val_acc: 0.3611\n",
      "Epoch 80/200\n",
      "5/5 [==============================] - 1s 299ms/step - loss: 0.2979 - acc: 0.9012 - val_loss: 2.8359 - val_acc: 0.3775\n",
      "Epoch 81/200\n",
      "5/5 [==============================] - 2s 300ms/step - loss: 0.2881 - acc: 0.9050 - val_loss: 2.8253 - val_acc: 0.3799\n",
      "Epoch 82/200\n",
      "5/5 [==============================] - 2s 305ms/step - loss: 0.2765 - acc: 0.9062 - val_loss: 2.8982 - val_acc: 0.3727\n",
      "Epoch 83/200\n",
      "5/5 [==============================] - 2s 302ms/step - loss: 0.2731 - acc: 0.9124 - val_loss: 2.7125 - val_acc: 0.3814\n",
      "Epoch 84/200\n",
      "5/5 [==============================] - 2s 340ms/step - loss: 0.2844 - acc: 0.9036 - val_loss: 2.7684 - val_acc: 0.3782\n",
      "Epoch 85/200\n",
      "5/5 [==============================] - 2s 303ms/step - loss: 0.2622 - acc: 0.9116 - val_loss: 2.9359 - val_acc: 0.3690\n",
      "Epoch 86/200\n",
      "5/5 [==============================] - 2s 305ms/step - loss: 0.2637 - acc: 0.9104 - val_loss: 2.8747 - val_acc: 0.3678\n",
      "Epoch 87/200\n",
      "5/5 [==============================] - 1s 297ms/step - loss: 0.2657 - acc: 0.9106 - val_loss: 2.8540 - val_acc: 0.3837\n",
      "Epoch 88/200\n",
      "5/5 [==============================] - 1s 300ms/step - loss: 0.2631 - acc: 0.9120 - val_loss: 3.0164 - val_acc: 0.3725\n",
      "Epoch 89/200\n",
      "5/5 [==============================] - 2s 301ms/step - loss: 0.2490 - acc: 0.9148 - val_loss: 2.8113 - val_acc: 0.3791\n",
      "Epoch 90/200\n",
      "5/5 [==============================] - 2s 306ms/step - loss: 0.2468 - acc: 0.9194 - val_loss: 2.9841 - val_acc: 0.3617\n",
      "Epoch 91/200\n",
      "5/5 [==============================] - 2s 301ms/step - loss: 0.2485 - acc: 0.9162 - val_loss: 3.0572 - val_acc: 0.3640\n",
      "Epoch 92/200\n",
      "5/5 [==============================] - 2s 309ms/step - loss: 0.2505 - acc: 0.9240 - val_loss: 2.9743 - val_acc: 0.3778\n",
      "Epoch 93/200\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.2356 - acc: 0.9246 - val_loss: 2.8567 - val_acc: 0.3785\n",
      "Epoch 94/200\n",
      "5/5 [==============================] - 1s 297ms/step - loss: 0.2362 - acc: 0.9216 - val_loss: 3.2553 - val_acc: 0.3586\n",
      "Epoch 95/200\n",
      "5/5 [==============================] - 2s 324ms/step - loss: 0.2413 - acc: 0.9226 - val_loss: 2.9223 - val_acc: 0.3849\n",
      "Epoch 96/200\n",
      "5/5 [==============================] - 2s 302ms/step - loss: 0.2293 - acc: 0.9278 - val_loss: 3.1934 - val_acc: 0.3606\n",
      "Epoch 97/200\n",
      "5/5 [==============================] - 2s 342ms/step - loss: 0.2504 - acc: 0.9164 - val_loss: 3.1109 - val_acc: 0.3602\n",
      "Epoch 98/200\n",
      "5/5 [==============================] - 1s 296ms/step - loss: 0.2298 - acc: 0.9264 - val_loss: 3.1477 - val_acc: 0.3660\n",
      "Epoch 99/200\n",
      "5/5 [==============================] - 2s 301ms/step - loss: 0.2176 - acc: 0.9262 - val_loss: 2.9564 - val_acc: 0.3810\n",
      "Epoch 100/200\n",
      "5/5 [==============================] - 2s 306ms/step - loss: 0.2212 - acc: 0.9274 - val_loss: 2.9939 - val_acc: 0.3684\n",
      "Epoch 101/200\n",
      "5/5 [==============================] - 1s 297ms/step - loss: 0.2142 - acc: 0.9336 - val_loss: 3.1520 - val_acc: 0.3648\n",
      "Epoch 102/200\n",
      "5/5 [==============================] - 2s 300ms/step - loss: 0.2153 - acc: 0.9286 - val_loss: 3.0869 - val_acc: 0.3794\n",
      "Epoch 103/200\n",
      "2/5 [===========>..................] - ETA: 0s - loss: 0.2204 - acc: 0.9270"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-bd39b962f151>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# hist2 = model2.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhist2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_flow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_scale\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtarget_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m       \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[0;34m(generator, mode)\u001b[0m\n\u001b[1;32m    360\u001b[0m   \u001b[0;34m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf114/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf114/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf114/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf114/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# hist2 = model2.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "hist2 = model2.fit_generator(data_flow, epochs=epochs, steps_per_epoch=math.ceil(aug_scale * len(x_train) / batch_size), validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data evaluation\n",
    "test_loss2, test_accuracy2 = model2.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy: {:.4f}\".format(test_accuracy2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_histories = [hist.history['loss'], hist2.history['loss']]\n",
    "train_acc_histories = [hist.history['acc'], hist2.history['acc']]\n",
    "test_acc_histories = [hist.history['val_acc'], hist2.history['val_acc']]\n",
    "\n",
    "plot_fn(['model1','model2'], train_loss_histories, train_acc_histories, test_acc_histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
