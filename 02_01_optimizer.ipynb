{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "오늘 강의에서 우리는 Deep learning 모델을 최적화하는 다양한 방법들에 대해 배웠습니다.\n",
    "\n",
    "이번 실습을 통해 강의에서 배웠던 개념들을 일부 구현하고 실험해보도록 하겠습니다.\n",
    "\n",
    "이 노트북에서는 다음 내용들을 다룹니다.\n",
    "- 즉시 실행(Eager execution)\n",
    "- Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from time import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 즉시 실행(Eager execution)\n",
    "이번 실습에서는 **즉시 실행(Eager execution)** 을 이용해서 코드를 실행시켜보도록 하겠습니다.\n",
    "즉시 실행은 TensorFlow를 대화형 명령 스타일 로 프로그래밍 할 수 있도록 해줍니다. 그래프 생성 없이 연산을 즉시 실행하기 때문에 직관적으로 실행하고 디버그할 수 있습니다.\n",
    "\n",
    "간단히 말해서 placeholder와 Session 없이 코드를 짤 수 있게됩니다.\n",
    "\n",
    "자세한 내용은 [Tensorflow 즉시 실행 (Eager Execution) 가이드](https://github.com/tgjeon/TF-Eager-Execution-Guide-KR)를 참고해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session\n",
    "# x = tf.placeholder(tf.float32, shape=[1, 1])\n",
    "# m = tf.matmul(x, x)\n",
    "# with tf.Session() as sess:\n",
    "#   print(sess.run(m, feed_dict={x: [[2.]]})) # -> [[4.]]\n",
    "\n",
    "# Eager execution\n",
    "tf.enable_eager_execution() # 반드시 프로그램 실행 전에 실행되어야 합니다.\n",
    "\n",
    "x = [[2.]]\n",
    "m = tf.matmul(x, x)\n",
    "\n",
    "print(m)\n",
    "print(m.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 777\n",
    "np.random.seed(seed)\n",
    "tf.random.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Fashion MNIST 데이터를 이용해서 실험을 진행해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train = x_train.reshape([-1, 28*28]).astype(np.float32) / 255.\n",
    "y_train = y_train.astype(np.int64)\n",
    "x_test = x_test.reshape([-1, 28*28]).astype(np.float32) / 255.\n",
    "y_test = y_test.astype(np.int64)\n",
    "\n",
    "print(f\"input shape: {x_train.shape}\")\n",
    "print(f\"label shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 생성\n",
    "베이스 라인으로 사용될 모델을 생성하겠습니다.\n",
    "\n",
    "모델 생성 단계에서 weight initialization을 수행할 수 있습니다.\n",
    "다양한 initializer를 [Tensorflow initializer docs](https://www.tensorflow.org/api_docs/python/tf/initializers)에서 찾아볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \"\"\"\n",
    "    Fully connected 모델을 생성합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, n_out, n_hidden_unit, n_hidden_layer):\n",
    "        n_units = [n_in] + [n_hidden_unit] * n_hidden_layer + [n_out]\n",
    "        # 주어진 크기의 모델을 생성합니다.\n",
    "        self.weights, self.biases = [], []\n",
    "        for i, (n_in, n_out) in enumerate(zip(n_units[:-1], n_units[1:])):\n",
    "            # weight initialization\n",
    "            ## 1. 직접 입력\n",
    "            limit = math.sqrt(6 / n_in)\n",
    "            weight = tf.Variable(tf.random_uniform([n_in, n_out], -limit, limit))\n",
    "            bias = tf.Variable(tf.zeros([n_out]))\n",
    "            ## 2. initializer 활용\n",
    "#             initializer = tf.glorot_uniform_initializer()\n",
    "#             weight = tf.Variable(initializer([n_in, n_out]))\n",
    "#             bias = tf.Variable(initializer([n_out]))\n",
    "            # gradient를 계산하기 위해 학습 가능한 변수들을 저장해둡니다.\n",
    "            self.weights.append(weight)\n",
    "            self.biases.append(bias)\n",
    "\n",
    "        self.variables = self.weights + self.biases\n",
    "    def __call__(self, x):\n",
    "        layer = x\n",
    "        for i, (weight, bias) in enumerate(zip(self.weights, self.biases)): \n",
    "            layer = tf.matmul(layer, weight) + bias\n",
    "            if i < len(self.weights) - 1:\n",
    "                layer = tf.nn.relu(layer)\n",
    "        y_hat = layer\n",
    "        return y_hat\n",
    "# Model Parameters\n",
    "n_in = 28 * 28\n",
    "n_out = 10\n",
    "n_hidden_unit = 100\n",
    "n_hidden_layer = 2\n",
    "\n",
    "model = Model(n_in, n_out, n_hidden_unit, n_hidden_layer)\n",
    "\n",
    "# model test\n",
    "sample_x = tf.zeros([1, 28*28])\n",
    "sample_y_hat = model(sample_x)\n",
    "print(f\"output tensor: {sample_y_hat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance function\n",
    "학습 평가를 위해 사용되는 함수들입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, images, labels):\n",
    "    logits = model(images)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=labels)\n",
    "    return loss\n",
    "\n",
    "def accuracy_fn(model, images, labels):\n",
    "    logits = model(images)\n",
    "    prediction = tf.equal(tf.argmax(logits, -1), labels)\n",
    "    accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "# gradient를 계산합니다.\n",
    "def grad(model, images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn(model, images, labels)\n",
    "    return tape.gradient(loss, model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "생성된 모델과 fashion mnist 데이터를 이용해 학습을 진행해보도록 하겠습니다.\n",
    "\n",
    "\n",
    "적절하다고 생각하는 batch_size를 정해서 **@TODO**를 채워주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Hyper-parameters\n",
    "batch_size = None #@TODO\n",
    "iterations = len(x_train) // batch_size\n",
    "\n",
    "epochs = 5\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# 학습을 위해 데이터를 배치 크기로 나눕니다.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(1000).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).shuffle(1000).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝 모델을 학습시키기 위해 사용되는 다양한 optimizer들이 있습니다.\n",
    "\n",
    "가장 기본적인 optimizer로는 (Stochastic) Gridient Descent 알고리즘을 기반으로 한 GradientDescentOptimizer가 있고\n",
    "대체로 가장 좋은 성능을 보인다고 알려진 optimizer는 강의에서 배운 AdamOptimizer가 있습니다.\n",
    "\n",
    "더 많은 optimizer들을 확인하기 위해서는 [Tensorflow optimizer docs](https://www.tensorflow.org/api_docs/python/tf/train/Optimizer)를 참고해주세요.\n",
    "\n",
    "사용할 optimizer를 정해서 **@TODO**를 채워주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = None #@TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loss_history = []\n",
    "train_acc_history = []\n",
    "test_acc_history = []\n",
    "start_time = time()\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "    for (idx, (images, labels)) in enumerate(train_dataset.take(iterations)):\n",
    "        # gradient를 계산하고 optimizer를 이용해 back propagation을 수행합니다.\n",
    "        grads = grad(model, images, labels)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables), global_step=tf.train.get_or_create_global_step())\n",
    "        \n",
    "        # performance를 계산합니다\n",
    "        train_loss = loss_fn(model, images, labels)\n",
    "        train_accuracy = accuracy_fn(model, images, labels)\n",
    "        test_accuracy = accuracy_fn(model, x_test, y_test)\n",
    "        \n",
    "        # 그래프를 그리기 위해 기록합니다.\n",
    "        train_loss_history.append(train_loss.numpy())\n",
    "        train_acc_history.append(train_accuracy.numpy())\n",
    "        test_acc_history.append(test_accuracy.numpy())\n",
    "        \n",
    "        # 학습 과정을 출력합니다.\n",
    "        if idx % 20 == 0 or idx == iterations - 1:\n",
    "            print(\"Epoch: [%2d] [%5d/%5d] time: %4.4f, train_loss: %.8f, train_accuracy: %.4f, test_Accuracy: %.4f\" \\\n",
    "                % (epoch + 1, idx + 1, iterations, time() - start_time, train_loss, train_accuracy,\n",
    "                   test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Result\n",
    "학습 과정을 그래프로 그려보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def plot_fn(labels, train_loss_histories, train_acc_histories, test_acc_histories):\n",
    "    fig = plt.figure(figsize=(18,5))\n",
    "    ax1 = fig.add_subplot(1, 3, 1)\n",
    "    ax2 = fig.add_subplot(1, 3, 2)\n",
    "    ax3 = fig.add_subplot(1, 3, 3)\n",
    "    for label, train_loss_history, train_acc_history, test_acc_history in zip(labels, train_loss_histories, train_acc_histories, test_acc_histories):\n",
    "        ax1.plot(train_loss_history, label=str(label))\n",
    "        ax2.plot(train_acc_history, label=str(label))\n",
    "        ax3.plot(test_acc_history, label=str(label))\n",
    "        \n",
    "    ax1.set_xlabel('Batch #')\n",
    "    ax1.set_ylabel('Training Loss [entropy]')\n",
    "    ax2.set_xlabel('Batch #')\n",
    "    ax2.set_ylabel('Training Accuracy')\n",
    "    ax3.set_xlabel('Batch #')\n",
    "    ax3.set_ylabel('Test Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "plot_fn(['Adam'],[train_loss_history], [train_acc_history], [test_acc_history])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "@TODO plot_fn을 이용해서 다양한 optimizer들의 학습 과정을 비교해보세요.\n",
    "\n",
    "힌트1: 위 학습 loop를 또 다른 loop로 감싸보세요.\n",
    "\n",
    "힌트2: \n",
    "\n",
    "optimizers={'sgd': tf.train.GradientDescentOptimizer(learning_rate),\n",
    "            'sgd w/ nesterov': tf.train.MomentumOptimizer(learning_rate, momentum=0.3 ,use_nesterov=True),\n",
    "            'adagrad': tf.train.AdagradOptimizer(learning_rate),\n",
    "            'rmsprop': tf.train.RMSPropOptimizer(learning_rate),\n",
    "            'adam': tf.train.AdamOptimizer(learning_rate)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 심화\n",
    "몇몇 optimizer의 경우 hyperparameter들이 있습니다.(eg. Adam: beta1=0.9, beta2=0.999, epsilon=1e-08)\n",
    "\n",
    "weight initialization을 조정해보고 \n",
    "optimizer와 optimizer의 hyperparameter를 조정하면서 가장 높은 test accuracy를 구해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
